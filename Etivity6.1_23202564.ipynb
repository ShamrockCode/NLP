{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUflM/gh6JRMkkNh1gDdmL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Name: Feiyang Li  \n","ID: 23202564"],"metadata":{"id":"8u3_I3FJcGQi"}},{"cell_type":"markdown","source":["# Task 1"],"metadata":{"id":"05fXQcVwdCze"}},{"cell_type":"markdown","source":["a. Microaveraged precision:   \n","\\begin{align}\n","        P_{\\mu} = \\frac{870}{870+230} = 0.791.\n","    \\end{align}\n","\n","b. Microaveraged Recall:\n","\\begin{align}\n","        R_{\\mu} = \\frac{870}{870+230} = 0.791.\n","    \\end{align}\n","\n","c. Microaveraged F1:\n","\\begin{align}\n","        F1_{\\mu} = 2 \\cdot \\frac{P_{\\mu} \\cdot R_{\\mu}}{P_{\\mu} + R_{\\mu}} = 0.791\n","    \\end{align}\n","\n","d. Macroaveraged Precision:  \n","\\begin{align}\n","        P_M = \\frac{1}{2} \\left( \\frac{800}{800+200} + \\frac{70}{70+30} \\right) = 0.75\n","    \\end{align}\n","e. Macroaveraged Recall:  \n","\\begin{align}\n","        R_M = \\frac{1}{2} \\left( \\frac{800}{800+200} + \\frac{70}{70+30} \\right) = 0.75\n","    \\end{align}  \n","f. Macroaveraged F1:  \n","\\begin{align}\n","        F1_M = 2 \\cdot \\frac{P_M \\cdot R_M}{P_M + R_M} = 0.75\n","    \\end{align}"],"metadata":{"id":"nMMiAYDFcBHq"}},{"cell_type":"markdown","source":["Discussion: In a word, since the sample size of the \"Food\" category is much larger than that of the \"Drink\" category, the performance of the \"Food\" category has a greater impact on the overall value in the microaveraged calculation. It can be seen that the value for the microaveraged are higher than the macroaveraged. This is because the microaveraged takes into account all categorization results, whereas the macroaveraged gives equal weight to each category, regardless of its sample size. This explains why the F1 value of the microaverage is higher than the macroaverage."],"metadata":{"id":"4lcvUAz5sMY9"}},{"cell_type":"markdown","source":["# Task 3"],"metadata":{"id":"hhoVo863hYr_"}},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","def sentimentAnalyzer(text):\n","    blob = TextBlob(text)\n","    polarity = blob.sentiment.polarity\n","    subjectivity = blob.sentiment.subjectivity\n","    if polarity > 0.1:\n","      # The polarity of last sample is 0.1, and its output is netural.\n","      # I am not sure if I should set threshold 0.1 and -0.1\n","        sentiment = \"positive sentiment 😊\"\n","    elif polarity < -0.1:\n","        sentiment = \"negative sentiment 😠\"\n","    else:\n","        sentiment = \"netural sentiment 😐\"\n","\n","    print(f\"String= {text}\\n\")\n","    print(f\"Sentiment(polarity={polarity}, subjectivity={subjectivity})\")\n","    print(f\"{sentiment}\\nSubjectivity: {subjectivity}\\n\")\n","    print(\"------------------------------------\")\n","\n","sample_sentences = [\n","    \"NLP is cool\",\n","    \"NLP is cool and useful\",\n","    \"NLP is hard\",\n","    \"NLP is hard and useless\",\n","    \"NLP stands for Natural Language Processing\"\n","]\n","\n","for sentence in sample_sentences:\n","    sentimentAnalyzer(sentence)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPHzhurehaea","executionInfo":{"status":"ok","timestamp":1700835253703,"user_tz":0,"elapsed":218,"user":{"displayName":"Fiona Li","userId":"11737142977593193829"}},"outputId":"ef58aef5-029b-4498-a259-3e2183d4490f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["String= NLP is cool\n","\n","Sentiment(polarity=0.35, subjectivity=0.65)\n","positive sentiment 😊\n","Subjectivity: 0.65\n","\n","------------------------------------\n","String= NLP is cool and useful\n","\n","Sentiment(polarity=0.32499999999999996, subjectivity=0.325)\n","positive sentiment 😊\n","Subjectivity: 0.325\n","\n","------------------------------------\n","String= NLP is hard\n","\n","Sentiment(polarity=-0.2916666666666667, subjectivity=0.5416666666666666)\n","negative sentiment 😠\n","Subjectivity: 0.5416666666666666\n","\n","------------------------------------\n","String= NLP is hard and useless\n","\n","Sentiment(polarity=-0.39583333333333337, subjectivity=0.37083333333333335)\n","negative sentiment 😠\n","Subjectivity: 0.37083333333333335\n","\n","------------------------------------\n","String= NLP stands for Natural Language Processing\n","\n","Sentiment(polarity=0.1, subjectivity=0.4)\n","netural sentiment 😐\n","Subjectivity: 0.4\n","\n","------------------------------------\n"]}]},{"cell_type":"markdown","source":["# Task 2"],"metadata":{"id":"zpGXNRHKkEOU"}},{"cell_type":"code","source":["import math\n","from collections import Counter\n","\n","def naiveBayesClassifier(training_set, test_set):\n","    class_counts = Counter([label for _, label in training_set])\n","    total_docs = len(training_set)\n","    priors = {label: count / total_docs for label, count in class_counts.items()}\n","\n","    word_freq = {label: Counter() for label in class_counts.keys()}\n","    for doc, label in training_set:\n","        word_freq[label].update(doc.split())\n","\n","    total_word_count = {label: sum(freq.values()) for label, freq in word_freq.items()}\n","    vocabulary = set(word for words in word_freq.values() for word in words)\n","\n","    def calculate_probability(doc, label):\n","        words = doc.split()\n","        log_prob = 0\n","        for word in words:\n","            word_probability = (word_freq[label].get(word, 0) + 1) / (total_word_count[label] + len(vocabulary))\n","            log_prob += math.log(word_probability)\n","        return log_prob\n","\n","    for doc, _ in test_set:\n","        probabilities = {\n","            label: math.log(prior) + calculate_probability(doc, label)\n","            for label, prior in priors.items()\n","        }\n","        predicted_class = max(probabilities, key=probabilities.get)\n","        sentiment = \"POSITIVE\" if predicted_class == '+' else \"NEGATIVE\"\n","        print(f\"Doc: '{doc}' \\n Predicting sentimental classification: {sentiment}\")\n","\n","\n","\n","\n","trainingSet = [('just plain boring','-'),('entirely predictable and lacks energy','-'),('no surprises and very few laughs','-'),('very powerful','+'),('the most fun film of the summer','+')]\n","testSet = [('predictable with no fun','?')]\n","naiveBayesClassifier(trainingSet,testSet)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6OhnGkDVkGSH","executionInfo":{"status":"ok","timestamp":1700835253913,"user_tz":0,"elapsed":13,"user":{"displayName":"Fiona Li","userId":"11737142977593193829"}},"outputId":"942ca15a-60ab-43e1-a405-7063a3575644"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Doc: 'predictable with no fun' \n"," Predicting sentimental classification: NEGATIVE\n"]}]}]}